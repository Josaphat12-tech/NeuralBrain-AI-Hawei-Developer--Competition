"""
Prediction Scheduler - Hourly Update Loop

Runs predictions every hour using APScheduler:
1. Fetch latest disease data
2. Run GPT predictions
3. Generate alerts
4. Update database
5. Log execution

Survives API failures - uses graceful fallbacks
Survives Flask restarts - uses persistent scheduling
"""

import logging
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger
from datetime import datetime, timedelta
import os
import atexit

logger = logging.getLogger(__name__)

class PredictionScheduler:
    """Manages hourly prediction scheduling"""
    
    _scheduler = None
    _is_running = False
    _app = None  # Store Flask app reference
    
    @classmethod
    def init_scheduler(cls, app=None):
        """Initialize the scheduler with Flask app context"""
        try:
            if cls._scheduler is None:
                cls._scheduler = BackgroundScheduler(daemon=True)
                
                # Add hourly prediction job (runs every hour)
                cls._scheduler.add_job(
                    func=cls._run_predictions_with_context,
                    trigger=IntervalTrigger(hours=1),
                    id='hourly_predictions',
                    name='Hourly AI Predictions',
                    replace_existing=True,
                    misfire_grace_time=900  # 15 minutes
                )
                
                logger.info("âœ… Scheduler initialized (BackgroundScheduler)")
                
                # Store app reference for context
                if app:
                    cls._app = app
                    cls._scheduler.start()
                    cls._is_running = True
                    logger.info("âœ… Scheduler started (hourly cycle active)")
                    logger.info("âœ… Next prediction scheduled for 1 hour from now")
                    logger.info("âœ… Scheduler is resilient to API failures (will retry next hour)")
                    
                    # Register graceful shutdown on app termination ONLY
                    # Don't use atexit - let Flask handle shutdown
                else:
                    logger.warning("âš ï¸ No Flask app provided to scheduler")
                    
        except Exception as e:
            logger.error(f"âŒ Scheduler initialization error: {str(e)}")
            import traceback
            traceback.print_exc()
    
    @classmethod
    def _run_predictions_with_context(cls):
        """Run predictions within Flask app context"""
        if not hasattr(cls, '_app'):
            logger.error("âŒ No Flask app context available")
            return
            
        with cls._app.app_context():
            cls._run_predictions()
    
    @classmethod
    def _run_predictions(cls):
        """Execute full prediction pipeline"""
        try:
            start_time = datetime.utcnow()
            logger.info("\n" + "="*60)
            logger.info("ðŸ”® HOURLY PREDICTION CYCLE STARTING")
            logger.info("="*60)
            
            # Import services here to avoid circular imports
            from services.disease_data_service import DiseaseDataService
            from services.prediction_service import PredictionService
            from services.alert_engine import AlertEngine
            from services.data_normalizer import DataNormalizer
            
            # 1. Fetch real data
            logger.info("\nðŸ“Š STEP 1: Fetching real disease data...")
            global_stats = DiseaseDataService.get_global_stats()
            countries = DiseaseDataService.get_countries_data()
            historical = DiseaseDataService.get_historical_data(days=60)
            regional_risks = DiseaseDataService.get_regional_outbreak_risk()
            
            logger.info(f"   âœ… Global: {global_stats.get('cases', 0)} cases")
            logger.info(f"   âœ… Countries: {len(countries)} regions")
            logger.info(f"   âœ… Historical: {len(historical)} days")
            logger.info(f"   âœ… Regional risks: {len(regional_risks)} high-risk areas")
            
            # 2. Generate predictions
            logger.info("\nðŸ¤– STEP 2: Generating GPT predictions...")
            predictor = PredictionService()
            predictions_7day = predictor.predict_outbreak_7_day(
                global_stats,
                countries,
                historical
            )
            regional_predictions = predictor.predict_regional_risk(countries)
            health_analytics = predictor.predict_health_analytics(global_stats, countries)
            
            logger.info(f"   âœ… 7-day forecast: {len(predictions_7day)} days")
            logger.info(f"   âœ… Regional predictions: {len(regional_predictions)} regions")
            logger.info(f"   âœ… Health analytics: {len(health_analytics)} metrics")
            
            # 3. Generate alerts
            logger.info("\nðŸš¨ STEP 3: Generating alerts...")
            alerts = AlertEngine.generate_alerts(
                global_stats,
                regional_predictions,
                predictions_7day,
                historical
            )
            
            logger.info(f"   âœ… Total alerts: {len(alerts)}")
            
            # 4. Log execution summary
            duration = (datetime.utcnow() - start_time).total_seconds()
            logger.info("\n" + "="*60)
            logger.info(f"âœ… PREDICTION CYCLE COMPLETE in {duration:.1f}s")
            logger.info("="*60 + "\n")
            
        except Exception as e:
            logger.error(f"\nâŒ PREDICTION CYCLE FAILED: {str(e)}\n", exc_info=True)
            logger.warning("âš ï¸ Scheduler will retry predictions in 1 hour (resilient to failures)")
    
    @classmethod
    def start(cls):
        """Start the scheduler"""
        try:
            if cls._scheduler and not cls._is_running:
                cls._scheduler.start()
                cls._is_running = True
                logger.info("âœ… Scheduler started")
        except Exception as e:
            logger.error(f"âŒ Scheduler start error: {str(e)}")
    
    @classmethod
    def stop(cls):
        """Stop the scheduler"""
        try:
            if cls._scheduler and cls._is_running:
                cls._scheduler.shutdown()
                cls._is_running = False
                logger.info("âœ… Scheduler stopped")
        except Exception as e:
            logger.error(f"âŒ Scheduler stop error: {str(e)}")
            logger.info("ðŸ”® HOURLY PREDICTION CYCLE STARTING")
            logger.info("="*60)
            
            # Import services here to avoid circular imports
            from services.disease_data_service import DiseaseDataService
            from services.prediction_service import PredictionService
            from services.alert_engine import AlertEngine
            from services.data_normalizer import DataNormalizer
            
            # 1. Fetch real data
            logger.info("\nðŸ“Š STEP 1: Fetching real disease data...")
            global_stats = DiseaseDataService.get_global_stats()
            countries = DiseaseDataService.get_countries_data()
            historical = DiseaseDataService.get_historical_data(days=60)
            regional_risks = DiseaseDataService.get_regional_outbreak_risk()
            
            logger.info(f"   âœ… Global: {global_stats.get('cases', 0)} cases")
            logger.info(f"   âœ… Countries: {len(countries)} regions")
            logger.info(f"   âœ… Historical: {len(historical)} days")
            logger.info(f"   âœ… Regional risks: {len(regional_risks)} high-risk areas")
            
            # 2. Generate predictions
            logger.info("\nðŸ¤– STEP 2: Generating GPT predictions...")
            predictor = PredictionService()
            predictions_7day = predictor.predict_outbreak_7_day(
                global_stats,
                countries,
                historical
            )
            regional_predictions = predictor.predict_regional_risk(countries)
            health_analytics = predictor.predict_health_analytics(global_stats, countries)
            
            logger.info(f"   âœ… 7-day forecast: {len(predictions_7day)} days")
            logger.info(f"   âœ… Regional predictions: {len(regional_predictions)} regions")
            logger.info(f"   âœ… Health analytics: {len(health_analytics)} metrics")
            
            # 3. Generate alerts
            logger.info("\nðŸš¨ STEP 3: Generating alerts...")
            alerts = AlertEngine.generate_alerts(
                global_stats,
                regional_predictions,
                predictions_7day,
                historical
            )
            
            logger.info(f"   âœ… Total alerts: {len(alerts)}")
            for alert in alerts:
                logger.info(f"      [{alert.get('type')}] {alert.get('title')}")
            
            # 4. Normalize data for frontend
            logger.info("\nðŸ“¦ STEP 4: Normalizing data for frontend...")
            dashboard_metrics = DataNormalizer.normalize_dashboard_metrics(global_stats)
            chart_data = DataNormalizer.normalize_chart_data(historical)
            map_data = DataNormalizer.normalize_map_data(regional_predictions)
            alerts_normalized = DataNormalizer.normalize_alerts(alerts)
            predictions_normalized = DataNormalizer.normalize_predictions(predictions_7day)
            analytics_normalized = DataNormalizer.normalize_analytics(health_analytics)
            
            logger.info("   âœ… Dashboard metrics normalized")
            logger.info("   âœ… Chart data normalized")
            logger.info("   âœ… Map data normalized")
            logger.info("   âœ… Alerts normalized")
            
            # 5. Store in database/cache
            logger.info("\nðŸ’¾ STEP 5: Storing predictions...")
            cls._store_predictions({
                'dashboard_metrics': dashboard_metrics,
                'chart_data': chart_data,
                'map_data': map_data,
                'alerts': alerts_normalized,
                'predictions': predictions_normalized,
                'analytics': analytics_normalized,
                'timestamp': datetime.utcnow().isoformat()
            })
            logger.info("   âœ… All data stored successfully")
            
            # 6. Log execution summary
            duration = (datetime.utcnow() - start_time).total_seconds()
            logger.info("\n" + "="*60)
            logger.info(f"âœ… PREDICTION CYCLE COMPLETE in {duration:.1f}s")
            logger.info("="*60 + "\n")
            
        except Exception as e:
            logger.error(f"\nâŒ PREDICTION CYCLE FAILED: {str(e)}\n", exc_info=True)
    
    @classmethod
    def _store_predictions(cls, data: dict):
        """Store predictions in database or cache"""
        try:
            # Store in file-based cache for now
            import json
            cache_file = os.path.join(
                os.path.dirname(__file__),
                '..',
                'cache',
                'latest_predictions.json'
            )
            
            # Create cache directory if needed
            os.makedirs(os.path.dirname(cache_file), exist_ok=True)
            
            with open(cache_file, 'w') as f:
                json.dump(data, f, indent=2, default=str)
            
            logger.info(f"   ðŸ“ Cache updated: {cache_file}")
            
        except Exception as e:
            logger.error(f"   âŒ Cache storage error: {str(e)}")
    
    @classmethod
    def get_latest_predictions(cls) -> dict:
        """Retrieve latest predictions from cache"""
        try:
            import json
            cache_file = os.path.join(
                os.path.dirname(__file__),
                '..',
                'cache',
                'latest_predictions.json'
            )
            
            if os.path.exists(cache_file):
                with open(cache_file, 'r') as f:
                    return json.load(f)
            
        except Exception as e:
            logger.error(f"âŒ Cache retrieval error: {str(e)}")
        
        return {}
    
    @classmethod
    def get_scheduler_status(cls) -> dict:
        """Get scheduler status"""
        if cls._scheduler is None:
            return {"status": "not_initialized"}
        
        jobs = []
        if cls._scheduler.get_jobs():
            for job in cls._scheduler.get_jobs():
                jobs.append({
                    "id": job.id,
                    "name": job.name,
                    "next_run": job.next_run_time.isoformat() if job.next_run_time else None,
                    "enabled": not job.paused
                })
        
        return {
            "status": "running" if cls._is_running else "stopped",
            "jobs": jobs,
            "running": cls._is_running
        }
